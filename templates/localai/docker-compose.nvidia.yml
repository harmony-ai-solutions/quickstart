services:
  localai:
    image: localai/localai:latest-gpu-nvidia-cuda-12
    pull_policy: always
    env_file: ../../../../localai/.env
    ports:
      - "${HOST_PORT:-8080}:${CONTAINER_PORT:-8080}"
    volumes:
      # - ./config.yml:/app/harmony-speech-engine/config.yml
      - ../../../../localai/models:${MODELS_PATH:-/models}
      - ../../../../localai/images/:/tmp/generated/images/
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
    networks:
      - harmony-link-network
    labels:
      com.docker.compose.project: harmony-link-integration-localai
      com.docker.compose.service: localai

networks:
  harmony-link-network:
    external: true
    name: harmony-link-network
