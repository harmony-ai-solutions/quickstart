services:
  vllm:
    image: rocm/vllm:latest
    pull_policy: always
    env_file: .env
    # IPC & shared memory settings: Required for WSL multi-gpu
    ipc: host
    shm_size: '8gb'
    ports:
      - "${BIND_IP:-0.0.0.0}:${HOST_PORT:-8000}:${CONTAINER_PORT:-8000}"
    volumes:
      - ../../../../shared/cache:/root/.cache
      - ../../../../shared/models:/root/models
      # WSL2: Override interfaces to allow GPU access
      - /usr/lib/wsl/lib/libdxcore.so:/usr/lib/libdxcore.so
    devices:
      - "/dev/dxg" # WSL2
    command: >
      --model ${VLLM_MODEL:-NousResearch/Meta-Llama-3.1-8B-Instruct}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE:-1}
      ${VLLM_EXTRA_ARGS}
    networks:
      - harmony-link-network
    labels:
      com.docker.compose.project: harmony-link-integration-vllm
      com.docker.compose.service: vllm

networks:
  harmony-link-network:
    external: true
