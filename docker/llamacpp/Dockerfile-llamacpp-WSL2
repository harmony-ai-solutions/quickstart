# Dockerfile for llama.cpp with ROCm/HIP support
# Builds on top of base ROCm 6.4.4 + Python 3.12 image
# Optimized for WSL2 with AMD RDNA 1-4 discrete GPUs

# Use the base image (build it first or pull from registry)
# To build base: cd ../base && docker build -f Dockerfile-ROCm-6_4_4-Python-3_12-WSL2 -t harmonyai/base-rocm-wsl2:6.4.4-py3.12-wsl .
FROM harmonyai/base-rocm-wsl2:6.4.4-py3.12-wsl2

# GPU architectures to compile for (RDNA 1-4 discrete GPUs)
# RDNA 1: gfx1010 (RX 5700/XT), gfx1012 (RX 5500/XT)
# RDNA 2: gfx1030 (RX 6800/XT/6900 XT), gfx1031 (RX 6700 XT)
# RDNA 3: gfx1100 (RX 7900 XTX/XT/GRE)
# RDNA 4: gfx1200 (RX 9060/XT), gfx1201 (RX 9070/XT/GRE)
ARG GPU_TARGETS="gfx1010;gfx1012;gfx1030;gfx1031;gfx1100;gfx1200;gfx1201"

# Build configuration
ARG MAX_JOBS=16
ARG LLAMA_CPP_VERSION=master

# Set working directory
WORKDIR /app

# Allow statements and log messages to immediately appear
ENV PYTHONUNBUFFERED=True

# Set GPU architectures as environment variables for downstream builds
ENV PYTORCH_ROCM_ARCH=${GPU_TARGETS}
ENV GPU_TARGETS=${GPU_TARGETS}

# Configure ccache
ENV CCACHE_DIR=/root/.cache/ccache
ENV MAX_JOBS=${MAX_JOBS}

# Clone llama.cpp repository
RUN git clone https://github.com/ggml-org/llama.cpp /app/llama.cpp \
    && cd /app/llama.cpp \
    && git checkout ${LLAMA_CPP_VERSION}

WORKDIR /app/llama.cpp

# Build llama.cpp with HIP support
# Note: HIP_DEVICE_LIB_PATH is set to handle WSL2 ROCm device library path issues
# GPU_TARGETS is inherited from base image
RUN set -eux; \
    export HIPCXX="$(hipconfig -l)/clang"; \
    export HIP_PATH="$(hipconfig -p)"; \
    export HIP_DEVICE_LIB_PATH="${HIP_PATH}/amdgcn/bitcode"; \
    cmake -S . -B build \
        -DCMAKE_BUILD_TYPE=Release \
        -DCMAKE_C_COMPILER=clang \
        -DCMAKE_CXX_COMPILER=clang++ \
        -DGGML_HIP=ON \
        -DGPU_TARGETS="${GPU_TARGETS}" \
        -DGGML_HIP_ROCWMMA_FATTN=ON \
        #-DGGML_CURL=ON \
        #-DLLAMA_CURL=ON \
    && cmake --build build --config Release -- -j ${MAX_JOBS}

# Install the built binaries
RUN cp build/bin/* /usr/local/bin/ 2>/dev/null || true \
    && cp build/lib*.so /usr/local/lib/ 2>/dev/null || true

# Create necessary directories for models and cache
RUN mkdir -p /models /cache

# Set environment variables for runtime
ENV LLAMA_CACHE=/cache
ENV HF_HOME=/cache

# Copy entrypoint script
COPY entrypoint.sh /app/entrypoint.sh
RUN chmod +x /app/entrypoint.sh

# Expose default port
EXPOSE 8000

# Set entrypoint
ENTRYPOINT ["/app/entrypoint.sh"]

# Default command (can be overridden)
CMD ["--help"]

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Labels
LABEL maintainer="Harmony AI Solutions"
LABEL description="llama.cpp server with ROCm/HIP support for AMD RDNA 1-4 GPUs on WSL2"
LABEL base.image="harmonyai/base-rocm:6.4.4-py3.12-wsl2"
LABEL rocm.version="6.4.4"
LABEL python.version="3.12"
LABEL llama.cpp.version="${LLAMA_CPP_VERSION}"
LABEL gpu.targets="gfx1010,gfx1012,gfx1030,gfx1031,gfx1100,gfx1200,gfx1201"
