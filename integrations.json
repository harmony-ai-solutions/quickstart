[
  {
    "name": "text-generation-webui",
    "displayName": "Text Generation WebUI",
    "description": "Local text generation web interface and OpenAI Compatible API Server",
    "projectWebsite": "https://github.com/harmony-ai-solutions/text-generation-webui-harmony-ai",
    "apiPath": "/v1",
    "webPath": "/",
    "configFiles": {
      "cpu": [
        {
          "name": "CMD_FLAGS.txt",
          "path": "text-generation-webui/CMD_FLAGS.txt",
          "description": "Command line flags and model configuration",
          "type": "txt",
          "required": false
        }
      ],
      "nvidia": [
        {
          "name": "CMD_FLAGS.txt",
          "path": "text-generation-webui/CMD_FLAGS.txt",
          "description": "Command line flags and model configuration",
          "type": "txt",
          "required": false
        }
      ]
    },
    "compatibleProviders": {
      "backend": ["openaicompatible"],
      "countenance": ["openaicompatible"],
      "movement": ["openaicompatible"],
      "rag": ["openaicompatible"]
    }
  },
  {
    "name": "harmony-speech-engine",
    "displayName": "Harmony Speech Engine",
    "description": "Local speech synthesis and recognition engine",
    "projectWebsite": "https://github.com/harmony-ai-solutions/harmony-speech-engine",
    "apiPath": "/v1",
    "configFiles": {
      "cpu": [
        {
          "name": "config.yml",
          "path": "harmony-speech-engine/config.yml",
          "description": "Model configuration for CPU inference",
          "type": "yaml",
          "required": true
        }
      ],
      "nvidia": [
        {
          "name": "config.yml",
          "path": "harmony-speech-engine/config.nvidia.yml",
          "description": "Model configuration for NVIDIA GPU inference",
          "type": "yaml",
          "required": true
        }
      ]
    },
    "compatibleProviders": {
      "tts": ["harmonyspeech"],
      "stt": ["harmonyspeech"]
    }
  },
  {
    "name": "localai",
    "displayName": "LocalAI",
    "description": "Drop-In Framework for local AI Services in the style of OpenAI. Supports LLMs, Neural Encoders, TTS & STT Pipelines",
    "projectWebsite": "https://localai.io/",
    "apiPath": "/v1",
    "webPath": "/",
    "configFiles": {
      "cpu": []
    },
    "compatibleProviders": {
      "backend": ["openaicompatible"],
      "countenance": ["openaicompatible"],
      "movement": ["openaicompatible"],
      "rag": ["openaicompatible"],
      "tts": ["openaicompatible"],
      "stt": ["openaicompatible"]
    }
  },
  {
    "name": "ollama",
    "displayName": "Ollama",
    "description": "Drop-In Framework for local AI Services in the style of OpenAI. Supports LLMs & Neural Encoders",
    "projectWebsite": "https://ollama.com/",
    "apiPath": "/v1",
    "webPath": "/",
    "configFiles": {
      "cpu": []
    },
    "compatibleProviders": {
      "backend": ["openaicompatible"],
      "countenance": ["openaicompatible"],
      "movement": ["openaicompatible"],
      "rag": ["openaicompatible"]
    }
  }
]
