# Specifies the CUDA devices visible for the container
# ID 0 usually is the primary GPU; if running multiple GPUs you may want to specify this more precisely
CUDA_VISIBLE_DEVICES=0
# CUDA_VISIBLE_DEVICES=1,2

# NCCL settings for WSL - comment in if running WSL and you're having issues
#NCCL_DEBUG=INFO
#NCCL_P2P_DISABLE=1  # Disable peer-to-peer if causing issues in WSL
#NCCL_SHM_DISABLE=0  # Ensure shared memory is enabled

# IP to bind the ports on - use 0.0.0.0 for IPv4 and :: for IPv6
BIND_IP=0.0.0.0
# the port the llama.cpp server binds to on the host
HOST_PORT=8000
# the port the llama.cpp server binds to inside the container
CONTAINER_PORT=8000

# Path to the GGUF model file (relative to /models mount point)
# Example: MODEL_PATH=/models/your-model.gguf
MODEL_PATH=/models/L3-8B-Stheno-v3.2-Q8_0-imat.gguf

# Default server configuration
# Context size (in tokens)
LLAMA_CTX_SIZE=8192
# Number of threads (CPU only, GPU variants use GPU offload)
LLAMA_N_THREADS=2

# GPU offload configuration (for GPU variants)
# Number of GPU layers to offload (0 = CPU only, -1 = all layers)
LLAMA_N_GPU_LAYERS=-1
# Main GPU to use for offload
LLAMA_MAIN_GPU=0

# Additional llama.cpp server arguments (space-separated)
# These will be passed directly to the llama-server command
LLAMA_EXTRA_ARGS=
# LLAMA_EXTRA_ARGS=--override-tensor "token\_embd.weight=CUDA0" --tensor-split 50,50
# source: https://www.reddit.com/r/LocalLLaMA/comments/1kpe33n/speed_up_llamacpp_on_uneven_multigpu_setups_rtx/

# Set APP_RUNTIME_GID to an appropriate host system group to enable access to mounted volumes
# You can find your current host user group id with the command `id -g`
APP_RUNTIME_GID=6972
